
 U-Net Variants Benchmark (CNN Â· Transformers Â· Mamba)

A clean, reproducible benchmark framework for **binary medical image segmentation**, supporting multiple public datasets and a growing collection of Uâ€‘Net variants (CNN-based, Transformer-based and Mamba-based).

This project is **inspired by the engineering patterns developed during my M.Sc. thesis**, but it is a **new, broader, independent framework** focused on:

- extensibility  
- reproducibility  
- fair model comparison  
- modular research workflows  
- clean engineering design

---

## ğŸš€ Objectives

- Provide a unified and reproducible pipeline for **training / evaluation / inference**  
- Enable rapid experimentation with different **Uâ€‘Net architectures**  
- Offer a modular framework that researchers and engineers can easily extend  
- Support multi-dataset benchmarking with consistent metrics and preprocessing  

---

## ğŸ§© Supported Model Families

### **CNN-Based**
- U-Net
- (soon) ResNetâ€‘Uâ€‘Net

### **Transformer-Based**
- (upcoming) Swin-UNet  

### **Mamba-Based**
- (upcoming) VMâ€‘UNet  

---

## âœ¨ Key Features

### **Unified Experiment Runner**
- Build models from Hydra config files  
- Automatic parameter count  
- FLOPs estimation (input-size dependent)  
- Detailed `torchinfo` model summaries  
- Train / Evaluate with consistent metrics  
- Save / Load checkpoints  

### **Experiment Tracking**
- MLflow logging  
- Metrics, parameters, curves  
- Artifacts (model weights, visual outputs, summaries)  

### **Modular Architecture**
- Dataset wrappers  
- Dataloaders  
- Training / evaluation engines  
- Model factory  
- Inspection utilities (FLOPs, summaries, visualizations)

---

## ğŸ“š Supported Datasets

Place datasets inside the `data/` folder following this structure:

Currently supported:

- **ISIC2017**
- (upcoming)**Kvasir-SEG**
- (upcoming)**BUSI**

---

## ğŸ“ Evaluation Metrics

- **Dice Similarity Coefficient (DSC)**
- **Mean Intersection over Union (mIoU)**
- **Accuracy**
- **Sensitivity**
- **Specificity**

---

## ğŸ§± Project Architecture Overview
```markdown

unet_variants/
â”œâ”€ data/                              # Data storage
â”‚  â””â”€ datasets
â”œâ”€ configs/
â”‚  â”œâ”€ config.yaml                     # Top-level Hydra config; composes model/data/train/inspect/logging/task/paths
â”‚  â”œâ”€ data/
â”‚  â”‚  â””â”€ isic.yaml                    # Example dataset config (paths, input size, normalization)
â”‚  â”œâ”€ eval/
â”‚  â”‚  â””â”€ default.yaml                 # Common evaluation parameters
â”‚  â”œâ”€ inspect/
â”‚  â”‚  â””â”€ default.yaml                 # Common model inspection parameters
â”‚  â”œâ”€ logging/
â”‚  â”‚   â””â”€ mlflow.yaml                 # Tracking URI, experiment name, run naming
â”‚  â”œâ”€ model/
â”‚  â”‚   â””â”€ unet.yaml                   # U-Net config
â”‚  â”œâ”€ task/
â”‚  â”‚   â””â”€ default.yaml                # Segmentation task (Binary, output channels)  
â”‚  â””â”€ train/
â”‚  â”‚   â””â”€ default.yaml                # Common training params (optimizer, scheduler, loss, batch_size)
â”œâ”€ src/
â”‚  â”œâ”€ unet_variants/
â”‚  â”‚  â”œâ”€ data/                        # Dataset modules (loaders, transforms, preparation)
â”‚  â”‚  â”‚  â”œâ”€â”€ dataset.py
â”‚  â”‚  â”‚  â”œâ”€â”€ loaders.py
â”‚  â”‚  â”‚  â”œâ”€â”€ transforms.py
â”‚  â”‚  â”‚  â””â”€â”€ prepare.py
â”‚  â”‚  â”œâ”€ engine/                      # Core training & evaluation logic
â”‚  â”‚  â”‚  â”œâ”€â”€ trainer.py
â”‚  â”‚  â”‚  â”œâ”€â”€ evaluator.py
â”‚  â”‚  â”‚  â”œâ”€â”€ inference.py
â”‚  â”‚  â”‚  â””â”€â”€ checkpoint.py   
â”‚  â”œâ”€â”€ inspection/                     # Introspection and profiling utilities
â”‚  â”‚  â”‚   â”œâ”€â”€ flops.py
â”‚  â”‚  â”‚   â”œâ”€â”€ summary.py
â”‚  â”‚  â”‚   â”œâ”€â”€ viz.py
â”‚  â”‚  â”‚   â”œâ”€â”€ inspector.py
â”‚  â”‚  â”‚   â””â”€â”€ onnx.py
â”‚  â”‚  â”œâ”€â”€ losses/                      # Loss functions (BCE+Dice, etc.)
â”‚  â”‚  â”‚   â””â”€â”€ bce_dice.py
â”‚  â”‚  â”œâ”€â”€ metrics/                     # Metrics for segmentation evaluation
â”‚  â”‚  â”‚   â””â”€â”€ segmentation.py
â”‚  â”‚  â”œâ”€â”€ runners/                     # Experiment runner (Hydra + MLflow)
â”‚  â”‚  â”‚   â””â”€â”€ experiment.py
â”‚  â”‚  â”œâ”€â”€ utils/                       # General-purpose utilities
â”‚  â”‚  â”‚   â”œâ”€â”€ bootstrap.py
â”‚  â”‚  â”‚   â”œâ”€â”€ device.py
â”‚  â”‚  â”‚   â”œâ”€â”€ io.py
â”‚  â”‚  â”‚   â”œâ”€â”€ logging.py
â”‚  â”‚  â”‚   â””â”€â”€ seeds.py
â”‚  â”‚  â”œâ”€â”€ models/                      # All U-Net variants live here
â”‚  â”‚  â”‚   â”œâ”€â”€ components/              # Reusable blocks (conv blocks, attention, upsample)
â”‚  â”‚  â”‚   â”œâ”€â”€ unet/                    # Baseline U-Net implementation
â”‚  â”‚  â”‚   â””â”€â”€ factory.py               # ğŸ”‘ Model registry/factory (maps string keys â†’ model classes)
â”œâ”€ scripts/
â”‚  â”œâ”€ run_train.sh
â”‚  â””â”€ run_eval.sh
â”œâ”€ runs/
â”‚  â”œâ”€ hydraruns                       
â”‚  â””â”€ mlruns
â”œâ”€ .gitignore
â”œâ”€ README.md
â”œâ”€ LICENSE
â””â”€ pyproject.toml                 # Packaging + minimal dependencies
```

## âš™ï¸ Installation

### 1. Create environment
```
conda create --name unet-benchmark python=3.11
conda activate unet-benchmark
```

### 2. Install PyTorch + CUDA
```
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu124
```

### 3. Install package locally
```
pip install -e .
```


## ğŸš€ Getting Started

After installing the environment and the package, you can immediately run an experiment.

### 1. Run a training experiment

```
python scripts/train.py model=unet dataset=isic2017
```

Hydra will automatically create a timestamped folder inside:
```markdown
runs/
â”‚  â”œâ”€â”€ hydra/
â”‚  â”œâ”€â”€ mlflow/
```
### 2. View MLflow dashboard
```
mlflow ui --backend-store-uri runs/mlflow
```
Open the URL to inspect:

- training curves
- model parameters
- artifacts (checkpoints, sample predictions)
= metrics across experiments

### 3. Run model inspection (FLOPs, summary)
```
python scripts/inspect.py model=unet input_size=1,3,256,256
```
This generates:

- FLOPs
- parameter count
- architecture summary
- optional visualizations

### 4. Perform inference
```
python scripts/infer.py model=unet ckpt=path/to/checkpoint.png input=path/to/image.png
```
The output will be saved inside:
```markdown
runs/
â”‚  â”œâ”€â”€ mlflow/
â”‚  â”‚  â”œâ”€â”€ <run_id>/
â”‚  â”‚  â”‚  â”œâ”€â”€ predictions/
```
